{
  "OpenAI": {
    "ServiceId": "text-davinci-003",
    "ModelId": "text-davinci-003",
    "ChatModelId": "gpt-3.5-turbo",
    "ApiKey": ""
  },
  "AzureOpenAI": {
    "ServiceId": "azure-text-davinci-003",
    "DeploymentName": "text-davinci-003",
    "ChatDeploymentName": "gpt-4",
    "Endpoint": "",
    "ApiKey": ""
  },
  "OpenAIEmbeddings": {
    "ServiceId": "text-embedding-ada-002",
    "ModelId": "text-embedding-ada-002",
    "ApiKey": ""
  },
  "AzureOpenAIEmbeddings": {
    "ServiceId": "azure-text-embedding-ada-002",
    "DeploymentName": "text-embedding-ada-002",
    "Endpoint": "",
    "ApiKey": ""
  },
  "HuggingFace": {
    "ApiKey": ""
  },
  "Bing": {
    "ApiKey": ""
  },
  "Postgres": {
    "ConnectionString": ""
  },
  "MultiConnector": {
    "OobaboogaEndPoint": "http://localhost",
    "GlobalParameters": {
      "SystemSupplement": "Assume you're about to engage in the 'semantic-function game'. In this context, every incoming prompt will be based on a semantic function, even if it's not perfectly formed or seems ambiguous. Your primary goal is to identify and execute the core function or intent of the message, filtering out noise or extraneous details. Treat the following prompt as a function and provide a direct, precise completion without added commentary. Prioritize the most likely and salient function based on the information presented. Be alert to cues, even if they're subtle or embedded, and strive to respond as accurately and succinctly as possible.",
      "UserPreamble": "Let's engage in a game: carefully heed the upcoming directives. Respond solely with a continuation of my message, abstaining from any extra remarks.",
      "SemanticRemarks": "Semantic functions are structured templates that work in tandem with other similar templates and native code-based functions. The native functions, in particular, demand a precise adherence to given instructions. They rely heavily on accurately parsed input parameters and lack mechanisms to sift through any unnecessary noise.\n\nWhen assessing the appropriateness of a response, it's crucial to be discerning. While instructions often present an example that outlines the desired response format, these examples may not always be exhaustive. Occasionally, they might even be overly simplistic, intended merely to keep the instructions concise. Thus, always ensure that you thoroughly understand and thoughtfully apply these instructions to generate a fitting answer to the given input."
    },
    "IncludedConnectors": [
      "TheBloke_orca_mini_3B-GGML",
      "togethercomputer_RedPajama-INCITE-Chat-3B-v1",
      "TheBloke_StableBeluga-7B-GGML",
      "TheBloke_StableBeluga-13B-GGML",
      "TheBloke_upstage-llama-30b-instruct-2048-GGML"
    ],
    "IncludedConnectorsDev": [
      // Use this property in your testsettings.development.json file, uncommenting the appropriate lines according to your multi-start configuration,
      // This allows enabling and disabling secondary completions without altering the main testsettings.json file.
      //"TheBloke_orca_mini_3B-GGML",
      //"togethercomputer_RedPajama-INCITE-Chat-3B-v1",
      //"TheBloke_StableBeluga-7B-GGML",
      //"TheBloke_StableBeluga-13B-GGML",
      //"TheBloke_upstage-llama-30b-instruct-2048-GGML"
    ],
    "OobaboogaCompletions": [
      {
        "Name": "TheBloke_orca_mini_3B-GGML",
        "BlockingPort": 5000,
        "StreamingPort": 5010,
        "MaxTokens": 2048,
        "CostPer1000Token": 0.0003,
        "PromptTransform": {
          "Template": "### System:\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\n{SystemSupplement}\n\n### User:\n{0}\n\n### Assistant:"
        }
      },
      {
        "Name": "togethercomputer_RedPajama-INCITE-Chat-3B-v1",
        "BlockingPort": 5001,
        "StreamingPort": 5011,
        "MaxTokens": 2048,
        "CostPer1000Token": 0.0004,
        "PromptTransform": {
          "Template": "<human>: {UserPreamble}[Instruction]\n{0}\n<bot>:"
        }
      },
      {
        "Name": "TheBloke_StableBeluga-7B-GGML",
        "BlockingPort": 5002,
        "StreamingPort": 5012,
        "MaxTokens": 4096,
        "CostPer1000Token": 0.0005,
        "PromptTransform": {
          "Template": "### System:\nThis is a system prompt, please behave and help the user.\n{SystemSupplement}\n{SemanticRemarks}\n\n### User:\n{0}\n\n### Assistant:"
        }
      },
      {
        "Name": "TheBloke_StableBeluga-13B-GGML",
        "BlockingPort": 5003,
        "StreamingPort": 5013,
        "MaxTokens": 4096,
        "CostPer1000Token": 0.0007,
        "PromptTransform": {
          "Template": "### System:\nThis is a system prompt, please behave and help the user.\n{SystemSupplement}\n{SemanticRemarks}\n\n### User:\n{0}\n\n### Assistant:"
        }
      },
      {
        "Name": "TheBloke_upstage-llama-30b-instruct-2048-GGML",
        //"EndPoint": "http://x.x.x.x", Most probably you won't host that model on the same machine as the tests runner and the rest of the models
        "BlockingPort": 5004,
        "StreamingPort": 5014,
        "MaxTokens": 2048,
        "CostPer1000Token": 0.001,
        "PromptTransform": {
          "Template": "### System:\r\n{SystemSupplement}\r\n\r\n### User:\r\n{0}\n\n### Assistant:\n"
        }
      }
    ]
  }
}
