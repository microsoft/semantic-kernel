{
  "query": "What is the Semantic Kernel?",
  "follow_up_questions": null,
  "answer": "Semantic Kernel is an open-source SDK for integrating AI models into applications. It allows developers to define plugins and orchestrate them with AI planners. It supports multiple programming languages like C#, Python, and Java.",
  "images": [
    {
      "url": "https://learn.microsoft.com/en-us/semantic-kernel/media/the-kernel-is-at-the-center-of-everything.png",
      "description": "The diagram illustrates the architecture of Microsoft's Semantic Kernel, detailing the processes of selecting AI services, rendering prompts, and invoking AI services, while also emphasizing aspects such as model selection, event notifications, responsible AI, and telemetry."
    },
    {
      "url": "https://learn.microsoft.com/en-us/semantic-kernel/media/kernel-infographic.png",
      "description": "A visual representation illustrates a kernel framework with various functionalities, including recall memory, task planning, Graph API integration, semantic functions, and native functions, along with an example of a task request for summarizing marketing tasks."
    },
    {
      "url": "https://user-images.githubusercontent.com/371009/221690156-3f90a8c9-ef90-46f7-a097-beb483656e97.png",
      "description": "A flowchart illustrates the process of handling user requests through stages of asking, planning, executing with AI skills and memory, and generating a result."
    },
    {
      "url": "https://user-images.githubusercontent.com/371009/221690406-caaff98e-87b5-40b7-9c58-cfa9623789b5.png",
      "description": "The diagram illustrates the integration of the Microsoft Semantic Kernel with Memory & Skills, highlighting components such as Skills & Abilities, Episodic Events, and Semantic Knowledge, along with their respective data sources and applications."
    },
    {
      "url": "https://learn.microsoft.com/en-us/semantic-kernel/media/mind-and-body-of-semantic-kernel.png",
      "description": "The diagram illustrates key components of semantic AI systems, including Models and Memory, Connectors, the Semantic Kernel, Plugins, and Triggers and Actions, organized in a hierarchical structure."
    }
  ],
  "results": [
    {
      "title": "Semantic Kernel overview for .NET - .NET | Microsoft Learn",
      "url": "https://learn.microsoft.com/en-us/dotnet/ai/semantic-kernel-dotnet-overview",
      "content": "Semantic Kernel is an open-source SDK that integrates and orchestrates AI models and services like OpenAI, Azure OpenAI, and Hugging Face with conventional programming languages like C#, Python, and Java. The following code snippet creates a Kernel and adds a connection to an Azure OpenAI model: Semantic Kernel plugins encapsulate standard language functions for applications and AI models to consume. Semantic functions are essentially AI prompts defined in your code that Semantic Kernel can customize and call as needed. The planner is a core component of Semantic Kernel that provides AI orchestration to manage seamless integration between AI models and plugins. Semantic Kernel's Vector stores provide abstractions over embedding models, vector databases, and other data to simplify context management for AI applications.",
      "score": 0.94989765,
      "raw_content": "Semantic Kernel overview for .NET - .NET | Microsoft Learn\nSkip to main content\nThis browser is no longer supported.\nUpgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.\nDownload Microsoft Edge More info about Internet Explorer and Microsoft Edge\nTable of contents Exit focus mode\nRead in English Save\nTable of contents Read in English Save Add to plan Edit\n\nShare via\nFacebook x.com LinkedIn Email\n\nPrint\nTable of contents\nSemantic Kernel overview for .NET\n\nArticle\n06/13/2024\n2 contributors\n\nFeedback\nIn this article\nIn this article, you explore Semantic Kernel core concepts and capabilities. Semantic Kernel is a powerful and recommended choice for working with AI in .NET applications. In the sections ahead, you learn:\n\nHow to add semantic kernel to your project\nSemantic Kernel core concepts\n\nThis article serves as an introductory overview of Semantic Kernel specifically in the context of .NET. For more comprehensive information and training about Semantic Kernel, see the following resources:\n\nSemantic Kernel documentation\nSemantic Kernel training\n\nAdd Semantic Kernel to a .NET project\nThe Semantic Kernel SDK is available as a NuGet package for .NET and integrates with standard app configurations.\nInstall the Microsoft.SemanticKernel package using the following command:\ndotnet add package Microsoft.SemanticKernel\nNote\nAlthough Microsoft.SemanticKernel provides core features of Semantic Kernel, additional capabilities require you to install additional packages. For example, the Microsoft.SemanticKernel.Plugins.Memory package provides to access memory related features. For more information, see the Semantic Kernel documentation.\nCreate and configure a Kernel instance using the KernelBuilder class to access and work with Semantic Kernel. The Kernel holds services, data, and connections to orchestrate integrations between your code and AI models.\nConfigure the Kernel in a .NET console app:\n```\nvar builder = Kernel.CreateBuilder();\n// Add builder configuration and services\nvar kernel = builder.Build();\n```\nConfigure the Kernel in an ASP.NET Core app:\n```\nvar builder = WebApplication.CreateBuilder();\nbuilder.Services.AddKernel();\n// Add builder configuration and services\nvar app = builder.Build();\n```\nUnderstand Semantic Kernel\nSemantic Kernel is an open-source SDK that integrates and orchestrates AI models and services like OpenAI, Azure OpenAI, and Hugging Face with conventional programming languages like C#, Python, and Java.\nThe Semantic Kernel SDK benefits enterprise developers in the following ways:\n\nStreamlines integration of AI capabilities into existing applications to enable a cohesive solution for enterprise products.\nMinimizes the learning curve of working with different AI models or services by providing abstractions that reduce complexity.\nImproves reliability by reducing the unpredictable behavior of prompts and responses from AI models. You can fine-tune prompts and plan tasks to create a controlled and predictable user experience.\n\nSemantic Kernel is built around several core concepts:\n\nConnections: Interface with external AI services and data sources.\nPlugins: Encapsulate functions that applications can use.\nPlanner: Orchestrates execution plans and strategies based on user behavior.\nMemory: Abstracts and simplifies context management for AI apps.\n\nThese building blocks are explored in more detail in the following sections.\nConnections\nThe Semantic Kernel SDK includes a set of connectors that enable developers to integrate LLMs and other services into their existing applications. These connectors serve as the bridge between the application code and the AI models or services. Semantic Kernel handles many common connection concerns and challenges for you so you can focus on building your own workflows and features.\nThe following code snippet creates a Kernel and adds a connection to an Azure OpenAI model:\n```\nusing Microsoft.SemanticKernel;\n// Create kernel\nvar builder = Kernel.CreateBuilder();\n// Add a chat completion service:\nbuilder.Services.AddAzureOpenAIChatCompletion(\n    \"your-resource-name\",\n    \"your-endpoint\",\n    \"your-resource-key\",\n    \"deployment-model\");\nvar kernel = builder.Build();\n```\nPlugins\nSemantic Kernel plugins encapsulate standard language functions for applications and AI models to consume. You can create your own plugins or rely on plugins provided by the SDK. These plugins streamline tasks where AI models are advantageous and efficiently combine them with more traditional C# methods. Plugin functions are generally categorized into two types: semantic functions and native functions.\nSemantic functions\nSemantic functions are essentially AI prompts defined in your code that Semantic Kernel can customize and call as needed. You can templatize these prompts to use variables, custom prompt and completion formatting, and more.\nThe following code snippet defines and registers a semantic function:\n```\nvar userInput = Console.ReadLine();\n// Define semantic function inline.\nstring skPrompt = @\"Summarize the provided unstructured text in a sentence that is easy to understand.\n                    Text to summarize: {{$userInput}}\";\n// Register the function\nkernel.CreateSemanticFunction(\n    promptTemplate: skPrompt,\n    functionName: \"SummarizeText\",\n    pluginName: \"SemanticFunctions\"\n);\n```\nNative functions\nNative functions are C# methods that Semantic Kernel can call directly to manipulate or retrieve data. They perform operations that are better suited for traditional code instructions instead of LLM prompts.\nThe following code snippet defines and registers a native function:\n```\n// Define native function\npublic class NativeFunctions {\n[SKFunction, Description(\"Retrieve content from local file\")]\npublic async Task<string> RetrieveLocalFile(string fileName, int maxSize = 5000)\n{\n    string content = await File.ReadAllTextAsync(fileName);\n    if (content.Length <= maxSize) return content;\n    return content.Substring(0, maxSize);\n}\n\n}\n//Import native function\nstring plugInName = \"NativeFunction\";\nstring functionName = \"RetrieveLocalFile\";\nvar nativeFunctions = new NativeFunctions();\nkernel.ImportFunctions(nativeFunctions, plugInName);\n```\nPlanner\nThe planner is a core component of Semantic Kernel that provides AI orchestration to manage seamless integration between AI models and plugins. This layer devises execution strategies from user requests and dynamically orchestrates Plugins to perform complex tasks with AI-assisted planning.\nConsider the following pseudo-code snippet:\n```\n// Native function definition and kernel configuration code omitted for brevity\n// Configure and create the plan\nstring planDefinition = \"Read content from a local file and summarize the content.\";\nSequentialPlanner sequentialPlanner = new SequentialPlanner(kernel);\nstring assetsFolder = @\"../../assets\";\nstring fileName = Path.Combine(assetsFolder,\"docs\",\"06_SemanticKernel\", \"aci_documentation.txt\");\nContextVariables contextVariables = new ContextVariables();\ncontextVariables.Add(\"fileName\", fileName);\nvar customPlan = await sequentialPlanner.CreatePlanAsync(planDefinition);\n// Execute the plan\nKernelResult kernelResult = await kernel.RunAsync(contextVariables, customPlan);\nConsole.WriteLine($\"Summarization: {kernelResult.GetValue()}\");\n```\nThe preceding code creates an executable, sequential plan to read content from a local file and summarize the content. The plan sets up instructions to read the file using a native function and then analyze it using an AI model.\nMemory\nSemantic Kernel's Vector stores provide abstractions over embedding models, vector databases, and other data to simplify context management for AI applications. Vector stores are agnostic to the underlying LLM or Vector database, offering a uniform developer experience. You can configure memory features to store data in a variety of sources or service, including Azure AI Search and Azure Cache for Redis.\nConsider the following code snippet:\n```\nvar facts = new Dictionary();\nfacts.Add(\n    \"Azure Machine Learning; https://learn.microsoft.com/en-us/azure/machine-learning/\",\n    @\"Azure Machine Learning is a cloud service for accelerating and\n    managing the machine learning project lifecycle. Machine learning professionals,\n    data scientists, and engineers can use it in their day-to-day workflows\"\n);\nfacts.Add(\n    \"Azure SQL Service; https://learn.microsoft.com/en-us/azure/azure-sql/\",\n    @\"Azure SQL is a family of managed, secure, and intelligent products\n    that use the SQL Server database engine in the Azure cloud.\"\n);\nstring memoryCollectionName = \"SummarizedAzureDocs\";\nforeach (var fact in facts) {\n    await memoryBuilder.SaveReferenceAsync(\n        collection: memoryCollectionName,\n        description: fact.Key.Split(\";\")[1].Trim(),\n        text: fact.Value,\n        externalId: fact.Key.Split(\";\")[2].Trim(),\n        externalSourceName: \"Azure Documentation\"\n    );\n}\n```\nThe preceding code loads a set of facts into memory so that the data is available to use when interacting with AI models and orchestrating tasks.\nQuickstart - Summarize text with OpenAI Quickstart - Chat with your data\nCollaborate with us on GitHub\nThe source for this content can be found on GitHub, where you can also create and review issues and pull requests. For more information, see our contributor guide.\n \n.NET\nOpen a documentation issue Provide product feedback\n\nAdditional resources\n\nYour Privacy Choices\nTheme\n\nLight\nDark\n\nHigh contrast\n\n\nPrevious Versions\n\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2025\n\nAdditional resources\nIn this article\n\nYour Privacy Choices\nTheme\n\nLight\nDark\n\nHigh contrast\n\n\nPrevious Versions\n\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2025\n"
    },
    {
      "title": "Introduction to Semantic Kernel | Microsoft Learn",
      "url": "https://learn.microsoft.com/en-us/semantic-kernel/overview/",
      "content": "Download Microsoft Edge More info about Internet Explorer and Microsoft Edge Table of contents Exit focus mode Read in English Save Table of contents Read in English Save Add to plan Edit Share via Facebook x.com LinkedIn Email Print Table of contents Introduction to Semantic Kernel Article 06/24/2024 6 contributors Feedback In this article Semantic Kernel is a lightweight, open-source development kit that lets you easily build AI agents and integrate the latest AI models into your C#, Python, or Java codebase. It serves as an efficient middleware that enables rapid delivery of enterprise-grade solutions. Semantic Kernel was designed to be future proof, easily connecting your code to the latest AI models evolving with the technology as it advances. Automating business processes Semantic Kernel combines prompts with existing APIs to perform actions.",
      "score": 0.9341134,
      "raw_content": "Introduction to Semantic Kernel | Microsoft Learn\nSkip to main content\nThis browser is no longer supported.\nUpgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.\nDownload Microsoft Edge More info about Internet Explorer and Microsoft Edge\nTable of contents Exit focus mode\nRead in English Save\nTable of contents Read in English Save Add to plan Edit\n\nShare via\nFacebook x.com LinkedIn Email\n\nPrint\nTable of contents\nIntroduction to Semantic Kernel\n\nArticle\n06/24/2024\n6 contributors\n\nFeedback\nIn this article\nSemantic Kernel is a lightweight, open-source development kit that lets you easily build AI agents and integrate the latest AI models into your C#, Python, or Java codebase. It serves as an efficient middleware that enables rapid delivery of enterprise-grade solutions.\nEnterprise ready\nMicrosoft and other Fortune 500 companies are already leveraging Semantic Kernel because it’s flexible, modular, and observable. Backed with security enhancing capabilities like telemetry support, and hooks and filters so you’ll feel confident you’re delivering responsible AI solutions at scale.\nVersion 1.0+ support across C#, Python, and Java means it’s reliable, committed to non breaking changes. Any existing chat-based APIs are easily expanded to support additional modalities like voice and video.\nSemantic Kernel was designed to be future proof, easily connecting your code to the latest AI models evolving with the technology as it advances. When new models are released, you’ll simply swap them out without needing to rewrite your entire codebase.\n\nAutomating business processes\nSemantic Kernel combines prompts with existing APIs to perform actions. By describing your existing code to AI models, they’ll be called to address requests. When a request is made the model calls a function, and Semantic Kernel is the middleware translating the model's request to a function call and passes the results back to the model.\nModular and extensible\nBy adding your existing code as a plugin, you’ll maximize your investment by flexibly integrating AI services through a set of out-of-the-box connectors. Semantic Kernel uses OpenAPI specifications (like Microsoft 365 Copilot) so you can share any extensions with other pro or low-code developers in your company.\n\nGet started\nNow that you know what Semantic Kernel is, get started with the quick start guide. You’ll build agents that automatically call functions to perform actions faster than any other SDK out there.\nQuickly get started\n\nAdditional resources\n\nYour Privacy Choices\nTheme\n\nLight\nDark\n\nHigh contrast\n\n\nPrevious Versions\n\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2025\n\nAdditional resources\nIn this article\n\nYour Privacy Choices\nTheme\n\nLight\nDark\n\nHigh contrast\n\n\nPrevious Versions\n\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2025\n"
    },
    {
      "title": "GitHub - microsoft/semantic-kernel: Integrate cutting-edge LLM ...",
      "url": "https://github.com/microsoft/semantic-kernel",
      "content": "Skip to content Navigation Menu Product Solutions Resources Open Source Enterprise Pricing Sign in Sign up microsoft / semantic-kernel Public Notifications Fork 3.5k Star 23k Code Issues 352 Pull requests 25 Discussions Actions Projects 1 Security Insights microsoft/semantic-kernel main Code Folders and files Name Last commit message Last commit date Latest commit History 3,973 Commits .devcontainer .github .vscode docs dotnet java prompt_template_samples python .editorconfig .gitattributes .gitignore CODE_OF_CONDUCT.md COMMUNITY.md CONTRIBUTING.md FEATURE_MATRIX.md LICENSE README.md SECURITY.md TRANSPARENCY_FAQS.md Repository files navigation README Code of conduct MIT license Security Semantic Kernel Status Python .NET Overview Semantic Kernel is an SDK that integrates Large Language Models (LLMs) like OpenAI, Azure OpenAI, and Hugging Face with conventional programming languages like C#, Python, and Java. Semantic Kernel achieves this by allowing you to define plugins that can be chained together in just a few lines of code. What makes Semantic Kernel special, however, is its ability to automatically orchestrate plugins with AI. With Semantic Kernel planners, you can ask an LLM to generate a plan that achieves a user's unique goal. Afterwards, Semantic Kernel will execute the plan for the user.",
      "score": 0.91988057,
      "raw_content": "Skip to content\nNavigation Menu\nProduct\nSolutions\nResources\nOpen Source\nEnterprise\nPricing\nSign in\nSign up\nmicrosoft\n/\nsemantic-kernel\nPublic\nNotifications\nFork 3.5k\n Star 23k\nCode\nIssues\n352\nPull requests\n25\nDiscussions\nActions\nProjects\n1\nSecurity\nInsights\nmicrosoft/semantic-kernel\n main\nCode\nFolders and files\nName    Last commit message Last commit date\nLatest commit\n \nHistory\n3,973 Commits\n.devcontainer\n.github\n.vscode\ndocs\ndotnet\njava\nprompt_template_samples\npython\n.editorconfig\n.gitattributes\n.gitignore\nCODE_OF_CONDUCT.md\nCOMMUNITY.md\nCONTRIBUTING.md\nFEATURE_MATRIX.md\nLICENSE\nREADME.md\nSECURITY.md\nTRANSPARENCY_FAQS.md\nRepository files navigation\nREADME\nCode of conduct\nMIT license\nSecurity\nSemantic Kernel\nStatus\nPython\n.NET\nOverview\nSemantic Kernel is an SDK that integrates Large Language Models (LLMs) like OpenAI, Azure OpenAI, and Hugging Face with conventional programming languages like C#, Python, and Java. Semantic Kernel achieves this by allowing you to define plugins that can be chained together in just a few lines of code.\nWhat makes Semantic Kernel special, however, is its ability to automatically orchestrate plugins with AI. With Semantic Kernel planners, you can ask an LLM to generate a plan that achieves a user's unique goal. Afterwards, Semantic Kernel will execute the plan for the user.\nIt provides:\nabstractions for AI services (such as chat, text to images, audio to text, etc.) and memory stores\nimplementations of those abstractions for services from OpenAI, Azure OpenAI, Hugging Face, local models, and more, and for a multitude of vector databases, such as those from Chroma, Qdrant, Milvus, and Azure\na common representation for plugins, which can then be orchestrated automatically by AI\nthe ability to create such plugins from a multitude of sources, including from OpenAPI specifications, prompts, and arbitrary code written in the target language\nextensible support for prompt management and rendering, including built-in handling of common formats like Handlebars and Liquid\nand a wealth of functionality layered on top of these abstractions, such as filters for responsible AI, dependency injection integration, and more.\nSemantic Kernel is utilized by enterprises due to its flexibility, modularity and observability. Backed with security enhancing capabilities like telemetry support, and hooks and filters so you’ll feel confident you’re delivering responsible AI solutions at scale. Semantic Kernel was designed to be future proof, easily connecting your code to the latest AI models evolving with the technology as it advances. When new models are released, you’ll simply swap them out without needing to rewrite your entire codebase.\nPlease star the repo to show your support for this project!\nGetting started with Semantic Kernel\nThe Semantic Kernel SDK is available in C#, Python, and Java. To get started, choose your preferred language below. See the Feature Matrix for a breakdown of feature parity between our currently supported languages.\nUsing Semantic Kernel in C#  \nUsing Semantic Kernel in Python\nUsing Semantic Kernel in Java\nThe quickest way to get started with the basics is to get an API key from either OpenAI or Azure OpenAI and to run one of the C#, Python, and Java console applications/scripts below.\nFor C#:\nGo to the Quick start page here and follow the steps to dive in.\nAfter Installing the SDK, we advise you follow the steps and code detailed to write your first console app. \nFor Python:\nGo to the Quick start page here and follow the steps to dive in.\nYou'll need to ensure that you toggle to Python in the Choose a programming language table at the top of the page. \nFor Java:\nThe Java code is in the semantic-kernel-java repository. See semantic-kernel-java build for instructions on how to build and run the Java code.\nPlease file Java Semantic Kernel specific issues in the semantic-kernel-java repository.\nLearning how to use Semantic Kernel\nThe fastest way to learn how to use Semantic Kernel is with our C# and Python Jupyter notebooks. These notebooks demonstrate how to use Semantic Kernel with code snippets that you can run with a push of a button.\nGetting Started with C# notebook\nGetting Started with Python notebook\nOnce you've finished the getting started notebooks, you can then check out the main walkthroughs on our Learn site. Each sample comes with a completed C# and Python project that you can run locally.\n📖 Getting Started\n🔌 Detailed Samples\n💡 Concepts\nFinally, refer to our API references for more details on the C# and Python APIs:\nC# API reference\nPython API reference\nJava API reference (coming soon)\nVisual Studio Code extension: design semantic functions with ease\nThe Semantic Kernel extension for Visual Studio Code makes it easy to design and test semantic functions. The extension provides an interface for designing semantic functions and allows you to test them with the push of a button with your existing models and data.\nJoin the community\nWe welcome your contributions and suggestions to SK community! One of the easiest ways to participate is to engage in discussions in the GitHub repository. Bug reports and fixes are welcome!\nFor new features, components, or extensions, please open an issue and discuss with us before sending a PR. This is to avoid rejection as we might be taking the core in a different direction, but also to consider the impact on the larger ecosystem.\nTo learn more and get started:\nRead the documentation\nLearn how to contribute to the project\nAsk questions in the GitHub discussions\nAsk questions in the Discord community\nAttend regular office hours and SK community events\nFollow the team on our blog\nContributor Wall of Fame\nCode of Conduct\nThis project has adopted the Microsoft Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.\nLicense\nCopyright (c) Microsoft Corporation. All rights reserved.\nLicensed under the MIT license.\nAbout\nIntegrate cutting-edge LLM technology quickly and easily into your apps\naka.ms/semantic-kernel\nTopics\nsdk ai artificial-intelligence openai llm\nResources\n Readme\nLicense\n MIT license\nCode of conduct\n Code of conduct\nSecurity policy\n Security policy\n Activity\n Custom properties\nStars\n 23k stars\nWatchers\n 280 watching\nForks\n 3.5k forks\nReport repository\nReleases 171\ndotnet-1.36.1\nLatest\nFeb 7, 2025\n+ 170 releases\nPackages\n \n \n \nUsed by 1k\n+ 993\nContributors\n354\n+ 340 contributors\nLanguages\nC#\n68.9%\nPython\n29.0%\nJupyter Notebook\n2.0%\nHandlebars\n0.1%\nPowerShell\n0.0%\nMakefile\n0.0%\nFooter\n© 2025 GitHub, Inc.\nFooter navigation\nTerms\nPrivacy\nSecurity\nStatus\nDocs\nContact\nManage cookies\nDo not share my personal information"
    },
    {
      "title": "How to quickly start with Semantic Kernel | Microsoft Learn",
      "url": "https://learn.microsoft.com/en-us/semantic-kernel/get-started/quick-start-guide",
      "content": "from semantic_kernel.connectors.ai.chat_completion_client_base import ChatCompletionClientBase from semantic_kernel.contents.chat_history import ChatHistory To make it easier to get started building enterprise apps with Semantic Kernel, we've created a step-by-step that guides you through the process of creating a kernel and using it to interact with AI services. from semantic_kernel.connectors.ai.chat_completion_client_base import ChatCompletionClientBase from semantic_kernel.contents.chat_history import ChatHistory import com.microsoft.semantickernel.Kernel; Afterwards, we add the most important part of a kernel: the AI services that you want to use. Once you've created your plugin, you can add it to the kernel so the AI agent can access it. In this guide, you learned how to quickly get started with Semantic Kernel by building a simple AI agent that can interact with an AI service and run your code.",
      "score": 0.59344494,
      "raw_content": "How to quickly start with Semantic Kernel | Microsoft Learn\nSkip to main content\nThis browser is no longer supported.\nUpgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.\nDownload Microsoft Edge More info about Internet Explorer and Microsoft Edge\nTable of contents Exit focus mode\nRead in English Save\nTable of contents Read in English Save Add to plan Edit\n\nShare via\nFacebook x.com LinkedIn Email\n\nPrint\nTable of contents\nGetting started with Semantic Kernel\n\nArticle\n11/08/2024\n10 contributors\n\nFeedback\nIn this article\nIn just a few steps, you can build your first AI agent with Semantic Kernel in either Python, .NET, or Java. This guide will show you how to...\n\nInstall the necessary packages\nCreate a back-and-forth conversation with an AI\nGive an AI agent the ability to run your code\nWatch the AI create plans on the fly\n\nInstalling the SDK\nSemantic Kernel has several NuGet packages available. For most scenarios, however, you typically only need Microsoft.SemanticKernel.\nYou can install it using the following command:\ndotnet add package Microsoft.SemanticKernel\nFor the full list of Nuget packages, please refer to the supported languages article.\nInstructions for accessing the SemanticKernel Python package is available here. It's as easy as:\npip install semantic-kernel\nInstructions for accessing the SemanticKernel Java package is available here. It's as easy as:\n```\n\n\n\ncom.microsoft.semantic-kernel\nsemantickernel-bom\n${sk.version}\npom\nimport\n\n\n\n\n\ncom.microsoft.semantic-kernel\nsemantickernel-api\n\n\ncom.microsoft.semantic-kernel\nsemantickernel-aiservices-openai\n\n\n```\nQuickly get started with notebooks\nIf you're a Python or C# developer, you can quickly get started with our notebooks. These notebooks provide step-by-step guides on how to use Semantic Kernel to build AI agents.\n\nTo get started, follow these steps:\n\nClone the Semantic Kernel repo\nOpen the repo in Visual Studio Code\nNavigate to _/python/samples/getting_started\nOpen 00-getting-started.ipynb to get started setting your environment and creating your first AI agent!\n\nTo get started, follow these steps:\n\nClone the Semantic Kernel repo\nOpen the repo in Visual Studio Code\nNavigate to _/dotnet/notebooks\nOpen 00-getting-started.ipynb to get started setting your environment and creating your first AI agent!\n\nWriting your first console app\n\nCreate a new .NET Console project using this command:\n\ndotnet new console\n\nInstall the following .NET dependencies:\n\ndotnet add package Microsoft.SemanticKernel\ndotnet add package Microsoft.Extensions.Logging\ndotnet add package Microsoft.Extensions.Logging.Console\n\nReplace the content of the Program.cs file with this code:\n\n```\n// Import packages\nusing Microsoft.Extensions.DependencyInjection;\nusing Microsoft.Extensions.Logging;\nusing Microsoft.SemanticKernel;\nusing Microsoft.SemanticKernel.ChatCompletion;\nusing Microsoft.SemanticKernel.Connectors.OpenAI;\n// Populate values from your OpenAI deployment\nvar modelId = \"\";\nvar endpoint = \"\";\nvar apiKey = \"\";\n// Create a kernel with Azure OpenAI chat completion\nvar builder = Kernel.CreateBuilder().AddAzureOpenAIChatCompletion(modelId, endpoint, apiKey);\n// Add enterprise components\nbuilder.Services.AddLogging(services => services.AddConsole().SetMinimumLevel(LogLevel.Trace));\n// Build the kernel\nKernel kernel = builder.Build();\nvar chatCompletionService = kernel.GetRequiredService();\n// Add a plugin (the LightsPlugin class is defined below)\nkernel.Plugins.AddFromType(\"Lights\");\n// Enable planning\nOpenAIPromptExecutionSettings openAIPromptExecutionSettings = new() \n{\n    FunctionChoiceBehavior = FunctionChoiceBehavior.Auto()\n};\n// Create a history store the conversation\nvar history = new ChatHistory();\n// Initiate a back-and-forth chat\nstring? userInput;\ndo {\n    // Collect user input\n    Console.Write(\"User > \");\n    userInput = Console.ReadLine();\n// Add user input\nhistory.AddUserMessage(userInput);\n\n// Get the response from the AI\nvar result = await chatCompletionService.GetChatMessageContentAsync(\n    history,\n    executionSettings: openAIPromptExecutionSettings,\n    kernel: kernel);\n\n// Print the results\nConsole.WriteLine(\"Assistant > \" + result);\n\n// Add the message from the agent to the chat history\nhistory.AddMessage(result.Role, result.Content ?? string.Empty);\n\n} while (userInput is not null);\n```\n```\nimport asyncio\nfrom semantic_kernel import Kernel\nfrom semantic_kernel.utils.logging import setup_logging\nfrom semantic_kernel.functions import kernel_function\nfrom semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\nfrom semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\nfrom semantic_kernel.connectors.ai.chat_completion_client_base import ChatCompletionClientBase\nfrom semantic_kernel.contents.chat_history import ChatHistory\nfrom semantic_kernel.functions.kernel_arguments import KernelArguments\nfrom semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.azure_chat_prompt_execution_settings import (\n    AzureChatPromptExecutionSettings,\n)\nasync def main():\n    # Initialize the kernel\n    kernel = Kernel()\n# Add Azure OpenAI chat completion\nchat_completion = AzureChatCompletion(\n    deployment_name=\"your_models_deployment_name\",\n    api_key=\"your_api_key\",\n    base_url=\"your_base_url\",\n)\nkernel.add_service(chat_completion)\n\n# Set the logging level for  semantic_kernel.kernel to DEBUG.\nsetup_logging()\nlogging.getLogger(\"kernel\").setLevel(logging.DEBUG)\n\n# Add a plugin (the LightsPlugin class is defined below)\nkernel.add_plugin(\n    LightsPlugin(),\n    plugin_name=\"Lights\",\n)\n\n# Enable planning\nexecution_settings = AzureChatPromptExecutionSettings()\nexecution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n\n# Create a history of the conversation\nhistory = ChatHistory()\n\n# Initiate a back-and-forth chat\nuserInput = None\nwhile True:\n    # Collect user input\n    userInput = input(\"User > \")\n\n    # Terminate the loop if the user says \"exit\"\n    if userInput == \"exit\":\n        break\n\n    # Add user input to the history\n    history.add_user_message(userInput)\n\n    # Get the response from the AI\n    result = await chat_completion.get_chat_message_content(\n        chat_history=history,\n        settings=execution_settings,\n        kernel=kernel,\n    )\n\n    # Print the results\n    print(\"Assistant > \" + str(result))\n\n    # Add the message from the agent to the chat history\n    history.add_message(result)\n\nRun the main function\nif name == \"main\":\n    asyncio.run(main())\n```\n```\nOpenAIAsyncClient client = new OpenAIClientBuilder()\n    .credential(new AzureKeyCredential(AZURE_CLIENT_KEY))\n    .endpoint(CLIENT_ENDPOINT)\n    .buildAsyncClient();\n// Import the LightsPlugin\nKernelPlugin lightPlugin = KernelPluginFactory.createFromObject(new LightsPlugin(),\n    \"LightsPlugin\");\n// Create your AI service client\nChatCompletionService chatCompletionService = OpenAIChatCompletion.builder()\n    .withModelId(MODEL_ID)\n    .withOpenAIAsyncClient(client)\n    .build();\n// Create a kernel with Azure OpenAI chat completion and plugin\nKernel kernel = Kernel.builder()\n    .withAIService(ChatCompletionService.class, chatCompletionService)\n    .withPlugin(lightPlugin)\n    .build();\n// Add a converter to the kernel to show it how to serialise LightModel objects into a prompt\nContextVariableTypes\n    .addGlobalConverter(\n        ContextVariableTypeConverter.builder(LightModel.class)\n            .toPromptString(new Gson()::toJson)\n            .build());\n// Enable planning\nInvocationContext invocationContext = new InvocationContext.Builder()\n    .withReturnMode(InvocationReturnMode.LAST_MESSAGE_ONLY)\n    .withToolCallBehavior(ToolCallBehavior.allowAllKernelFunctions(true))\n    .build();\n// Create a history to store the conversation\nChatHistory history = new ChatHistory();\n// Initiate a back-and-forth chat\nScanner scanner = new Scanner(System.in);\nString userInput;\ndo {\n  // Collect user input\n  System.out.print(\"User > \");\nuserInput = scanner.nextLine();\n  // Add user input\n  history.addUserMessage(userInput);\n// Prompt AI for response to users input\n  List<ChatMessageContent<?>> results = chatCompletionService\n      .getChatMessageContentsAsync(history, kernel, invocationContext)\n      .block();\nfor (ChatMessageContent<?> result : results) {\n    // Print the results\n    if (result.getAuthorRole() == AuthorRole.ASSISTANT && result.getContent() != null) {\n      System.out.println(\"Assistant > \" + result);\n    }\n    // Add the message from the agent to the chat history\n    history.addMessage(result);\n  }\n} while (userInput != null && !userInput.isEmpty());\n```\nThe following back-and-forth chat should be similar to what you see in the console. The function calls have been added below to demonstrate how the AI leverages the plugin behind the scenes.\n| Role | Message |\n| --- | --- |\n| 🔵 User | Please toggle the light |\n| 🔴 Assistant (function call) | LightsPlugin.GetState() |\n| 🟢 Tool | off |\n| 🔴 Assistant (function call) | LightsPlugin.ChangeState(true) |\n| 🟢 Tool | on |\n| 🔴 Assistant | The light is now on |\nIf you're interested in understanding more about the code above, we'll break it down in the next section.\nUnderstanding the code\nTo make it easier to get started building enterprise apps with Semantic Kernel, we've created a step-by-step that guides you through the process of creating a kernel and using it to interact with AI services.\n\n\nIn the following sections, we'll unpack the above sample by walking through steps 1, 2, 3, 4, 6, 9, and 10. Everything you need to build a simple agent that is powered by an AI service and can run your code.\n\nImport packages\n\nAdd AI services\n\n\nEnterprise components ::: zone-end\n\nBuild the kernel\nAdd memory (skipped)\nAdd plugins\nCreate kernel arguments (skipped)\nCreate prompts (skipped)\nPlanning\nInvoke\n\n1) Import packages\nFor this sample, we first started by importing the following packages:\nusing Microsoft.SemanticKernel;\nusing Microsoft.SemanticKernel.ChatCompletion;\nusing Microsoft.SemanticKernel.Connectors.OpenAI;\n```\nimport asyncio\nfrom semantic_kernel import Kernel\nfrom semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\nfrom semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\nfrom semantic_kernel.connectors.ai.chat_completion_client_base import ChatCompletionClientBase\nfrom semantic_kernel.contents.chat_history import ChatHistory\nfrom semantic_kernel.functions.kernel_arguments import KernelArguments\nfrom semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.azure_chat_prompt_execution_settings import (\n    AzureChatPromptExecutionSettings,\n)\n```\nimport com.microsoft.semantickernel.Kernel;\nimport com.microsoft.semantickernel.aiservices.openai.chatcompletion.OpenAIChatCompletion;\nimport com.microsoft.semantickernel.contextvariables.ContextVariableTypeConverter;\nimport com.microsoft.semantickernel.contextvariables.ContextVariableTypes;\nimport com.microsoft.semantickernel.orchestration.InvocationContext;\nimport com.microsoft.semantickernel.orchestration.InvocationReturnMode;\nimport com.microsoft.semantickernel.orchestration.ToolCallBehavior;\nimport com.microsoft.semantickernel.plugin.KernelPlugin;\nimport com.microsoft.semantickernel.plugin.KernelPluginFactory;\nimport com.microsoft.semantickernel.services.chatcompletion.AuthorRole;\nimport com.microsoft.semantickernel.services.chatcompletion.ChatCompletionService;\nimport com.microsoft.semantickernel.services.chatcompletion.ChatHistory;\nimport com.microsoft.semantickernel.services.chatcompletion.ChatMessageContent;\n2) Add AI services\nAfterwards, we add the most important part of a kernel: the AI services that you want to use. In this example, we added an Azure OpenAI chat completion service to the kernel builder.\nNote\nIn this example, we used Azure OpenAI, but you can use any other chat completion service. To see the full list of supported services, refer to the supported languages article. If you need help creating a different service, refer to the AI services article. There, you'll find guidance on how to use OpenAI or Azure OpenAI models as services.\n// Create kernel\nvar builder = Kernel.CreateBuilder()\nbuilder.AddAzureOpenAIChatCompletion(modelId, endpoint, apiKey);\n```\nInitialize the kernel\nkernel = Kernel()\nAdd Azure OpenAI chat completion\nkernel.add_service(AzureChatCompletion(\n    deployment_name=\"your_models_deployment_name\",\n    api_key=\"your_api_key\",\n    base_url=\"your_base_url\",\n))\n```\n```\n// Create your AI service client\nChatCompletionService chatCompletionService = OpenAIChatCompletion.builder()\n    .withModelId(MODEL_ID)\n    .withOpenAIAsyncClient(client)\n    .build();\n// Create a kernel with Azure OpenAI chat completion and plugin\nKernel kernel = Kernel.builder()\n    .withAIService(ChatCompletionService.class, chatCompletionService)\n    .withPlugin(lightPlugin)\n    .build();\n```\n3) Add enterprise services\nOne of the main benefits of using Semantic Kernel is that it supports enterprise-grade services. In this sample, we added the logging service to the kernel to help debug the AI agent.\nbuilder.Services.AddLogging(services => services.AddConsole().SetMinimumLevel(LogLevel.Trace));\n```\nimport logging\nSet the logging level for  semantic_kernel.kernel to DEBUG.\nlogging.basicConfig(\n    format=\"[%(asctime)s - %(name)s:%(lineno)d - %(levelname)s] %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n)\nlogging.getLogger(\"kernel\").setLevel(logging.DEBUG)\n```\n4) Build the kernel and retrieve services\nOnce the services have been added, we then build the kernel and retrieve the chat completion service for later use.\n```\nKernel kernel = builder.Build();\n// Retrieve the chat completion service\nvar chatCompletionService = kernel.Services.GetRequiredService();\n```\nOnce the kernel has been configured, we then retrieve the chat completion service for later use.\nNote\nIn Python, you don't need to explicitly build the kernel. Instead, you can access the services directly from the kernel object.\nchat_completion : AzureChatCompletion = kernel.get_service(type=ChatCompletionClientBase)\n// Create a kernel with Azure OpenAI chat completion and plugin\nKernel kernel = Kernel.builder()\n    .withAIService(ChatCompletionService.class, chatCompletionService)\n    .withPlugin(lightPlugin)\n    .build();\n6) Add plugins\nWith plugins, can give your AI agent the ability to run your code to retrieve information from external sources or to perform actions. In the above example, we added a plugin that allows the AI agent to interact with a light bulb. Below, we'll show you how to create this plugin.\nCreate a native plugin\nBelow, you can see that creating a native plugin is as simple as creating a new class.\nIn this example, we've created a plugin that can manipulate a light bulb. While this is a simple example, this plugin quickly demonstrates how you can support both...\n\nRetrieval Augmented Generation (RAG) by providing the AI agent with the state of the light bulb\nAnd task automation by allowing the AI agent to turn the light bulb on or off.\n\nIn your own code, you can create a plugin that interacts with any external service or API to achieve similar results.\n```\nusing System.ComponentModel;\nusing System.Text.Json.Serialization;\nusing Microsoft.SemanticKernel;\npublic class LightsPlugin\n{\n   // Mock data for the lights\n   private readonly List lights = new()\n   {\n      new LightModel { Id = 1, Name = \"Table Lamp\", IsOn = false },\n      new LightModel { Id = 2, Name = \"Porch light\", IsOn = false },\n      new LightModel { Id = 3, Name = \"Chandelier\", IsOn = true }\n   };\n[KernelFunction(\"get_lights\")]\n   [Description(\"Gets a list of lights and their current state\")]\n   public async Task> GetLightsAsync()\n   {\n      return lights;\n   }\n[KernelFunction(\"change_state\")]\n   [Description(\"Changes the state of the light\")]\n   public async Task ChangeStateAsync(int id, bool isOn)\n   {\n      var light = lights.FirstOrDefault(light => light.Id == id);\n  if (light == null)\n  {\n     return null;\n  }\n\n  // Update the light with the new state\n  light.IsOn = isOn;\n\n  return light;\n\n}\n}\npublic class LightModel\n{\n   [JsonPropertyName(\"id\")]\n   public int Id { get; set; }\n[JsonPropertyName(\"name\")]\n   public string Name { get; set; }\n[JsonPropertyName(\"is_on\")]\n   public bool? IsOn { get; set; }\n}\n```\n```\nfrom typing import Annotated\nfrom semantic_kernel.functions import kernel_function\nclass LightsPlugin:\n    lights = [\n        {\"id\": 1, \"name\": \"Table Lamp\", \"is_on\": False},\n        {\"id\": 2, \"name\": \"Porch light\", \"is_on\": False},\n        {\"id\": 3, \"name\": \"Chandelier\", \"is_on\": True},\n    ]\n@kernel_function(\n    name=\"get_lights\",\n    description=\"Gets a list of lights and their current state\",\n)\ndef get_state(\n    self,\n) -> str:\n    \"\"\"Gets a list of lights and their current state.\"\"\"\n    return self.lights\n\n@kernel_function(\n    name=\"change_state\",\n    description=\"Changes the state of the light\",\n)\ndef change_state(\n    self,\n    id: int,\n    is_on: bool,\n) -> str:\n    \"\"\"Changes the state of the light.\"\"\"\n    for light in self.lights:\n        if light[\"id\"] == id:\n            light[\"is_on\"] = is_on\n            return light\n    return None\n\n```\n```\npublic class LightsPlugin {\n// Mock data for the lights\n  private final Map lights = new HashMap<>();\npublic LightsPlugin() {\n    lights.put(1, new LightModel(1, \"Table Lamp\", false));\n    lights.put(2, new LightModel(2, \"Porch light\", false));\n    lights.put(3, new LightModel(3, \"Chandelier\", true));\n  }\n@DefineKernelFunction(name = \"get_lights\", description = \"Gets a list of lights and their current state\")\n  public List getLights() {\n    System.out.println(\"Getting lights\");\n    return new ArrayList<>(lights.values());\n  }\n@DefineKernelFunction(name = \"change_state\", description = \"Changes the state of the light\")\n  public LightModel changeState(\n      @KernelFunctionParameter(name = \"id\", description = \"The ID of the light to change\") int id,\n      @KernelFunctionParameter(name = \"isOn\", description = \"The new state of the light\") boolean isOn) {\n    System.out.println(\"Changing light \" + id + \" \" + isOn);\n    if (!lights.containsKey(id)) {\n      throw new IllegalArgumentException(\"Light not found\");\n    }\nlights.get(id).setIsOn(isOn);\n\nreturn lights.get(id);\n\n}\n}\n```\nAdd the plugin to the kernel\nOnce you've created your plugin, you can add it to the kernel so the AI agent can access it. In the sample, we added the LightsPlugin class to the kernel.\n// Add the plugin to the kernel\nkernel.Plugins.AddFromType<LightsPlugin>(\"Lights\");\n```\nAdd the plugin to the kernel\nkernel.add_plugin(\n    LightsPlugin(),\n    plugin_name=\"Lights\",\n)\n```\n// Import the LightsPlugin\nKernelPlugin lightPlugin = KernelPluginFactory.createFromObject(new LightsPlugin(),\n    \"LightsPlugin\");\n9) Planning\nSemantic Kernel leverages function calling–a native feature of most LLMs–to provide planning. With function calling, LLMs can request (or call) a particular function to satisfy a user's request. Semantic Kernel then marshals the request to the appropriate function in your codebase and returns the results back to the LLM so the AI agent can generate a final response.\nTo enable automatic function calling, we first need to create the appropriate execution settings so that Semantic Kernel knows to automatically invoke the functions in the kernel when the AI agent requests them.\nOpenAIPromptExecutionSettings openAIPromptExecutionSettings = new()\n{\n    FunctionChoiceBehavior = FunctionChoiceBehavior.Auto()\n};\nexecution_settings = AzureChatPromptExecutionSettings()\nexecution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n// Enable planning\nInvocationContext invocationContext = new InvocationContext.Builder()\n    .withReturnMode(InvocationReturnMode.LAST_MESSAGE_ONLY)\n    .withToolCallBehavior(ToolCallBehavior.allowAllKernelFunctions(true))\n    .build();\n10) Invoke\nFinally, we invoke the AI agent with the plugin. The sample code demonstrates how to generate a non-streaming response, but you can also generate a streaming response by using the GetStreamingChatMessageContentAsync method.\n```\n// Create chat history\nvar history = new ChatHistory();\n// Get the response from the AI\nvar result = await chatCompletionService.GetChatMessageContentAsync(\n    history,\n    executionSettings: openAIPromptExecutionSettings,\n    kernel: kernel\n);\n```\nRun the program using this command:\ndotnet run\n```\nCreate a history of the conversation\nhistory = ChatHistory()\nGet the response from the AI\nresult = (await chat_completion.get_chat_message_contents(\n    chat_history=history,\n    settings=execution_settings,\n    kernel=kernel,\n    arguments=KernelArguments(),\n))[0]\n```\n```\nuserInput = scanner.nextLine();\n// Add user input\nhistory.addUserMessage(userInput);\n// Prompt AI for response to users input\nList<ChatMessageContent<?>> results = chatCompletionService\n    .getChatMessageContentsAsync(history, kernel, invocationContext)\n    .block();\n```\nNext steps\nIn this guide, you learned how to quickly get started with Semantic Kernel by building a simple AI agent that can interact with an AI service and run your code. To see more examples and learn how to build more complex AI agents, check out our in-depth samples.\n\nAdditional resources\n\nYour Privacy Choices\nTheme\n\nLight\nDark\n\nHigh contrast\n\n\nPrevious Versions\n\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2025\n\nAdditional resources\nIn this article\n\nYour Privacy Choices\nTheme\n\nLight\nDark\n\nHigh contrast\n\n\nPrevious Versions\n\nBlog\nContribute\nPrivacy\nTerms of Use\nTrademarks\n© Microsoft 2025\n"
    }
  ],
  "response_time": 4.64
}