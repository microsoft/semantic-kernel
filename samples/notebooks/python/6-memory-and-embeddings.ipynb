{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68e1c158",
   "metadata": {},
   "source": [
    "# Building Semantic Memory with Embeddings\n",
    "\n",
    "So far, we've mostly been treating the kernel as a stateless orchestration engine.\n",
    "We send text into a model API and receive text out. \n",
    "\n",
    "In a [previous notebook](4-context-variables-chat.ipynb), we used `context variables` to pass in additional\n",
    "text into prompts to enrich them with more context. This allowed us to create a basic chat experience. \n",
    "\n",
    "However, if you solely relied on context variables, you would quickly realize that eventually your prompt\n",
    "would grow so large that you would run into a the model's token limit. What we need is a way to persist state\n",
    "and build both short-term and long-term memory to empower even more intelligent applications. \n",
    "\n",
    "To do this, we dive into the key concept of `Semantic Memory` in the Semantic Kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a77bdf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\abharris\\source\\repos\\microsoft\\semantic-kernel\\python\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: aiofiles<24.0.0,>=23.1.0 in c:\\users\\abharris\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from semantic-kernel==0.2.1.dev0) (23.1.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.24.2 in c:\\users\\abharris\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from semantic-kernel==0.2.1.dev0) (1.24.2)\n",
      "Requirement already satisfied: asyncio<4.0.0,>=3.4.3 in c:\\users\\abharris\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from semantic-kernel==0.2.1.dev0) (3.4.3)\n",
      "Requirement already satisfied: openai<0.28.0,>=0.27.0 in c:\\users\\abharris\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from semantic-kernel==0.2.1.dev0) (0.27.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\abharris\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai<0.28.0,>=0.27.0->semantic-kernel==0.2.1.dev0) (4.65.0)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\abharris\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai<0.28.0,>=0.27.0->semantic-kernel==0.2.1.dev0) (2.28.2)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\abharris\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai<0.28.0,>=0.27.0->semantic-kernel==0.2.1.dev0) (3.8.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abharris\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.20->openai<0.28.0,>=0.27.0->semantic-kernel==0.2.1.dev0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abharris\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.20->openai<0.28.0,>=0.27.0->semantic-kernel==0.2.1.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\abharris\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.20->openai<0.28.0,>=0.27.0->semantic-kernel==0.2.1.dev0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abharris\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.20->openai<0.28.0,>=0.27.0->semantic-kernel==0.2.1.dev0) (2022.12.7)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\abharris\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->openai<0.28.0,>=0.27.0->semantic-kernel==0.2.1.dev0) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\abharris\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->openai<0.28.0,>=0.27.0->semantic-kernel==0.2.1.dev0) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\abharris\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->openai<0.28.0,>=0.27.0->semantic-kernel==0.2.1.dev0) (22.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\abharris\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->openai<0.28.0,>=0.27.0->semantic-kernel==0.2.1.dev0) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\abharris\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->openai<0.28.0,>=0.27.0->semantic-kernel==0.2.1.dev0) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\abharris\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->openai<0.28.0,>=0.27.0->semantic-kernel==0.2.1.dev0) (6.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\abharris\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->openai<0.28.0,>=0.27.0->semantic-kernel==0.2.1.dev0) (0.4.6)\n",
      "Building wheels for collected packages: semantic-kernel\n",
      "  Building wheel for semantic-kernel (pyproject.toml): started\n",
      "  Building wheel for semantic-kernel (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for semantic-kernel: filename=semantic_kernel-0.2.1.dev0-py3-none-any.whl size=79776 sha256=eb4a2ff6d9725ee299be59c04487d237098a96408d41c691a88b932ab98c14d6\n",
      "  Stored in directory: C:\\Users\\abharris\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-yi3tziih\\wheels\\f0\\d9\\45\\71d1f022c671af9a2926da34224126647422d37b90ae6fef43\n",
      "Successfully built semantic-kernel\n",
      "Installing collected packages: semantic-kernel\n",
      "  Attempting uninstall: semantic-kernel\n",
      "    Found existing installation: semantic-kernel 0.2.1.dev0\n",
      "    Uninstalling semantic-kernel-0.2.1.dev0:\n",
      "      Successfully uninstalled semantic-kernel-0.2.1.dev0\n",
      "Successfully installed semantic-kernel-0.2.1.dev0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install ../../../python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "508ad44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel.ai.open_ai import OpenAITextCompletion, OpenAITextEmbedding, AzureTextCompletion, AzureTextEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ddffc1",
   "metadata": {},
   "source": [
    "In order to use memory, we need to instantiate the Kernel with a Memory Storage\n",
    "and an Embedding backend. In this example, we make use of the `VolatileMemoryStore` \"which can be thought of as a temporary in-memory storage (not to be confused with Semantic Memory). This memory is not written to disk and is only available during the app session.\n",
    "\n",
    "When developing your app you will have the option to plug in persistent storage like Azure Cosmos Db, PostgreSQL, SQLite, etc. Semantic Memory allows also to index external data sources, without duplicating all the information, more on that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f8dcbc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The Azure endpoint must start with https://",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mif\u001b[39;00m useAzureOpenAI:\n\u001b[0;32m      8\u001b[0m     deployment, api_key, endpoint \u001b[39m=\u001b[39m sk\u001b[39m.\u001b[39mazure_openai_settings_from_dot_env()\n\u001b[1;32m----> 9\u001b[0m     kernel\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39madd_text_backend(\u001b[39m\"\u001b[39m\u001b[39mdv\u001b[39m\u001b[39m\"\u001b[39m, AzureTextCompletion(\u001b[39m\"\u001b[39;49m\u001b[39mtext-davinci-003\u001b[39;49m\u001b[39m\"\u001b[39;49m, api_key, endpoint))\n\u001b[0;32m     10\u001b[0m     kernel\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39madd_embedding_backend(\u001b[39m\"\u001b[39m\u001b[39mada\u001b[39m\u001b[39m\"\u001b[39m, AzureTextEmbedding(\u001b[39m\"\u001b[39m\u001b[39mtext-embedding-ada-002\u001b[39m\u001b[39m\"\u001b[39m, api_key, endpoint))\n\u001b[0;32m     11\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\abharris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\semantic_kernel\\ai\\open_ai\\services\\azure_text_completion.py:56\u001b[0m, in \u001b[0;36mAzureTextCompletion.__init__\u001b[1;34m(self, deployment_name, endpoint, api_key, api_version, logger, ad_auth)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe Azure endpoint cannot be `None` or empty\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m endpoint\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mhttps://\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m---> 56\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe Azure endpoint must start with https://\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_endpoint \u001b[39m=\u001b[39m endpoint\n\u001b[0;32m     59\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_api_version \u001b[39m=\u001b[39m api_version\n",
      "\u001b[1;31mValueError\u001b[0m: The Azure endpoint must start with https://"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.kernel_config import KernelConfig\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "useAzureOpenAI = True\n",
    "\n",
    "# Configure AI backend used by the kernel\n",
    "if useAzureOpenAI:\n",
    "    deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "    kernel.config.add_text_backend(\"dv\", AzureTextCompletion(\"text-davinci-003\", endpoint, api_key))\n",
    "    kernel.config.add_embedding_backend(\"ada\", AzureTextEmbedding(\"text-embedding-ada-002\", endpoint, api_key))\n",
    "else:\n",
    "    api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "    kernel.config.add_text_backend(\"dv\", OpenAITextCompletion(\"text-davinci-003\", api_key, org_id))\n",
    "    kernel.config.add_embedding_backend(\"ada\", OpenAITextEmbedding(\"text-embedding-ada-002\", api_key, org_id))\n",
    "\n",
    "kernel.register_memory_store(memory_store=sk.memory.VolatileMemoryStore())\n",
    "kernel.import_skill(sk.core_skills.TextMemorySkill())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fefb6a",
   "metadata": {},
   "source": [
    "At its core, Semantic Memory is a set of data structures that allow you to store the meaning of text that come from different data sources, and optionally to store the source text too. These texts can be from the web, e-mail providers, chats, a database, or from your local directory, and are hooked up to the Semantic Kernel through data source connectors.\n",
    "\n",
    "The texts are embedded or compressed into a vector of floats representing mathematically the texts' contents and meaning. You can read more about embeddings [here](https://aka.ms/sk/embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7e7ca4",
   "metadata": {},
   "source": [
    "### Manually adding memories\n",
    "Let's create some initial memories \"About Me\". We can add memories to our `VolatileMemoryStore` by using `SaveInformationAsync`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d096504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def populate_memory(kernel: sk.Kernel) -> None:\n",
    "    # Add some documents to the semantic memory\n",
    "    await kernel.memory.save_information_async(\n",
    "        \"aboutMe\", id=\"info1\", text=\"My name is Andrea\"\n",
    "    )\n",
    "    await kernel.memory.save_information_async(\n",
    "        \"aboutMe\", id=\"info2\", text=\"I currently work as a tour guide\"\n",
    "    )\n",
    "    await kernel.memory.save_information_async(\n",
    "        \"aboutMe\", id=\"info3\", text=\"I've been living in Seattle since 2005\"\n",
    "    )\n",
    "    await kernel.memory.save_information_async(\n",
    "        \"aboutMe\", id=\"info4\", text=\"I visited France and Italy five times since 2015\"\n",
    "    )\n",
    "    await kernel.memory.save_information_async(\n",
    "        \"aboutMe\", id=\"info5\", text=\"My family is from New York\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caf8575",
   "metadata": {},
   "source": [
    "Let's try searching the memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628c843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def search_memory_examples(kernel: sk.Kernel) -> None:\n",
    "    questions = [\n",
    "        \"what's my name\",\n",
    "        \"where do I live?\",\n",
    "        \"where's my family from?\",\n",
    "        \"where have I traveled?\",\n",
    "        \"what do I do for work\",\n",
    "    ]\n",
    "\n",
    "    for question in questions:\n",
    "        print(f\"Question: {question}\")\n",
    "        result = await kernel.memory.search_async(\"aboutMe\", question)\n",
    "        print(f\"Answer: {result[0].text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70c2b22",
   "metadata": {},
   "source": [
    "Let's now revisit the our chat sample from the [previous notebook](4-context-variables-chat.ipynb).\n",
    "If you remember, we used context variables to fill the prompt with a `history` that continuously got populated as we chatted with the bot. Let's add also memory to it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed54a32",
   "metadata": {},
   "source": [
    "This is done by using the `TextMemorySkill` which exposes the `recall` native function.\n",
    "\n",
    "`recall` takes an input ask and performs a similarity search on the contents that have\n",
    "been embedded in the Memory Store and returns the most relevant memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8549b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def setup_chat_with_memory(\n",
    "    kernel: sk.Kernel,\n",
    ") -> Tuple[sk.SKFunctionBase, sk.SKContext]:\n",
    "    sk_prompt = \"\"\"\n",
    "    ChatBot can have a conversation with you about any topic.\n",
    "    It can give explicit instructions or say 'I don't know' if\n",
    "    it does not have an answer.\n",
    "\n",
    "    Information about me, from previous conversations:\n",
    "    - {{$fact1}} {{recall $fact1}}\n",
    "    - {{$fact2}} {{recall $fact2}}\n",
    "    - {{$fact3}} {{recall $fact3}}\n",
    "    - {{$fact4}} {{recall $fact4}}\n",
    "    - {{$fact5}} {{recall $fact5}}\n",
    "\n",
    "    Chat:\n",
    "    {{$chat_history}}\n",
    "    User: {{$user_input}}\n",
    "    ChatBot: \"\"\".strip()\n",
    "\n",
    "    chat_func = kernel.create_semantic_function(sk_prompt, max_tokens=200, temperature=0.8)\n",
    "\n",
    "    context = kernel.create_new_context()\n",
    "    context[\"fact1\"] = \"what is my name?\"\n",
    "    context[\"fact2\"] = \"where do I live?\"\n",
    "    context[\"fact3\"] = \"where's my family from?\"\n",
    "    context[\"fact4\"] = \"where have I traveled?\"\n",
    "    context[\"fact5\"] = \"what do I do for work?\"\n",
    "\n",
    "    context[sk.core_skills.TextMemorySkill.COLLECTION_PARAM] = \"aboutMe\"\n",
    "    context[sk.core_skills.TextMemorySkill.RELEVANCE_PARAM] = 0.8\n",
    "\n",
    "    context[\"chat_history\"] = \"\"\n",
    "\n",
    "    return chat_func, context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac62457",
   "metadata": {},
   "source": [
    "The `RelevanceParam` is used in memory search and is a measure of the relevance score from 0.0 to 1.0, where 1.0 means a perfect match. We encourage users to experiment with different values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645b55a1",
   "metadata": {},
   "source": [
    "Now that we've included our memories, let's chat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75267a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat(\n",
    "    kernel: sk.Kernel, chat_func: sk.SKFunctionBase, context: sk.SKContext\n",
    ") -> bool:\n",
    "    try:\n",
    "        user_input = input(\"User:> \")\n",
    "        context[\"user_input\"] = user_input\n",
    "        print(f\"User:> {user_input}\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "    except EOFError:\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "\n",
    "    if user_input == \"exit\":\n",
    "        print(\"\\n\\nExiting chat...\")\n",
    "        return False\n",
    "\n",
    "    answer = await kernel.run_on_vars_async(context.variables, chat_func)\n",
    "    context[\"chat_history\"] += f\"\\nUser:> {user_input}\\nChatBot:> {answer}\\n\"\n",
    "\n",
    "    print(f\"ChatBot:> {answer}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3875a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating memory...\n"
     ]
    },
    {
     "ename": "AIException",
     "evalue": "(<ErrorCodes.ServiceError: 6>, 'OpenAI service failed to generate embeddings', RateLimitError(message='Rate limit reached for microsoft_4x_enterprise_rate_limit in organization org-rocrupyvzgcl4yf25rqq6d1v on requests per min. Limit: 12000 / min. Please try again in 5ms. Contact support@openai.com if you continue to have issues.', http_status=429, request_id=None))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\abharris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\semantic_kernel\\ai\\open_ai\\services\\open_ai_text_embedding.py:64\u001b[0m, in \u001b[0;36mOpenAITextEmbedding.generate_embeddings_async\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m     response: Any \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopen_ai_instance\u001b[39m.\u001b[39mEmbedding\u001b[39m.\u001b[39macreate(\n\u001b[0;32m     65\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_args,\n\u001b[0;32m     66\u001b[0m         \u001b[39minput\u001b[39m\u001b[39m=\u001b[39mtexts,\n\u001b[0;32m     67\u001b[0m     )\n\u001b[0;32m     69\u001b[0m     \u001b[39m# make numpy arrays from the response\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abharris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_resources\\embedding.py:73\u001b[0m, in \u001b[0;36mEmbedding.acreate\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39macreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[39m# If a user specifies base64, we'll just return the encoded string.\u001b[39;00m\n\u001b[0;32m     76\u001b[0m     \u001b[39m# This is only for the default case.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abharris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:217\u001b[0m, in \u001b[0;36mEngineAPIResource.acreate\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    203\u001b[0m (\n\u001b[0;32m    204\u001b[0m     deployment_id,\n\u001b[0;32m    205\u001b[0m     engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m     api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    216\u001b[0m )\n\u001b[1;32m--> 217\u001b[0m response, _, api_key \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m requestor\u001b[39m.\u001b[39marequest(\n\u001b[0;32m    218\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m     url,\n\u001b[0;32m    220\u001b[0m     params\u001b[39m=\u001b[39mparams,\n\u001b[0;32m    221\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[0;32m    222\u001b[0m     stream\u001b[39m=\u001b[39mstream,\n\u001b[0;32m    223\u001b[0m     request_id\u001b[39m=\u001b[39mrequest_id,\n\u001b[0;32m    224\u001b[0m     request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    225\u001b[0m )\n\u001b[0;32m    227\u001b[0m \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    228\u001b[0m     \u001b[39m# must be an iterator\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abharris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_requestor.py:310\u001b[0m, in \u001b[0;36mAPIRequestor.arequest\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    300\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39marequest_raw(\n\u001b[0;32m    301\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    302\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    308\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    309\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_async_response(result, stream)\n\u001b[0;32m    311\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\abharris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_requestor.py:646\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_async_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    644\u001b[0m     util\u001b[39m.\u001b[39mlog_warn(e, body\u001b[39m=\u001b[39mresult\u001b[39m.\u001b[39mcontent)\n\u001b[0;32m    645\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 646\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    647\u001b[0m         (\u001b[39mawait\u001b[39;49;00m result\u001b[39m.\u001b[39;49mread())\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    648\u001b[0m         result\u001b[39m.\u001b[39;49mstatus,\n\u001b[0;32m    649\u001b[0m         result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    650\u001b[0m         stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    651\u001b[0m     ),\n\u001b[0;32m    652\u001b[0m     \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    653\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\abharris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_requestor.py:683\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 683\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    684\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    685\u001b[0m     )\n\u001b[0;32m    686\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Rate limit reached for microsoft_4x_enterprise_rate_limit in organization org-rocrupyvzgcl4yf25rqq6d1v on requests per min. Limit: 12000 / min. Please try again in 5ms. Contact support@openai.com if you continue to have issues.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAIException\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPopulating memory...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[39mawait\u001b[39;00m populate_memory(kernel)\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAsking questions... (manually)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[39mawait\u001b[39;00m search_memory_examples(kernel)\n",
      "Cell \u001b[1;32mIn[24], line 3\u001b[0m, in \u001b[0;36mpopulate_memory\u001b[1;34m(kernel)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mpopulate_memory\u001b[39m(kernel: sk\u001b[39m.\u001b[39mKernel) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m      2\u001b[0m     \u001b[39m# Add some documents to the semantic memory\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     \u001b[39mawait\u001b[39;00m kernel\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39msave_information_async(\n\u001b[0;32m      4\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maboutMe\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mid\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minfo1\u001b[39m\u001b[39m\"\u001b[39m, text\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMy name is Andrea\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m     )\n\u001b[0;32m      6\u001b[0m     \u001b[39mawait\u001b[39;00m kernel\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39msave_information_async(\n\u001b[0;32m      7\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maboutMe\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mid\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minfo2\u001b[39m\u001b[39m\"\u001b[39m, text\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mI currently work as a tour guide\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m     )\n\u001b[0;32m      9\u001b[0m     \u001b[39mawait\u001b[39;00m kernel\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39msave_information_async(\n\u001b[0;32m     10\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maboutMe\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mid\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minfo3\u001b[39m\u001b[39m\"\u001b[39m, text\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mI\u001b[39m\u001b[39m'\u001b[39m\u001b[39mve been living in Seattle since 2005\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\abharris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\semantic_kernel\\memory\\semantic_text_memory.py:31\u001b[0m, in \u001b[0;36mSemanticTextMemory.save_information_async\u001b[1;34m(self, collection, text, id, description)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39msave_information_async\u001b[39m(\n\u001b[0;32m     25\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     26\u001b[0m     collection: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     description: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     30\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 31\u001b[0m     embedding \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embeddings_generator\u001b[39m.\u001b[39mgenerate_embeddings_async([text])\n\u001b[0;32m     32\u001b[0m     data \u001b[39m=\u001b[39m MemoryRecord\u001b[39m.\u001b[39mlocal_record(\u001b[39mid\u001b[39m, text, description, embedding)\n\u001b[0;32m     34\u001b[0m     \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_storage\u001b[39m.\u001b[39mput_value_async(collection, \u001b[39mid\u001b[39m, data)\n",
      "File \u001b[1;32mc:\\Users\\abharris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\semantic_kernel\\ai\\open_ai\\services\\open_ai_text_embedding.py:73\u001b[0m, in \u001b[0;36mOpenAITextEmbedding.generate_embeddings_async\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[39mreturn\u001b[39;00m array(raw_embeddings)\n\u001b[0;32m     72\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mraise\u001b[39;00m AIException(\n\u001b[0;32m     74\u001b[0m         AIException\u001b[39m.\u001b[39mErrorCodes\u001b[39m.\u001b[39mServiceError,\n\u001b[0;32m     75\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mOpenAI service failed to generate embeddings\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     76\u001b[0m         ex,\n\u001b[0;32m     77\u001b[0m     )\n",
      "\u001b[1;31mAIException\u001b[0m: (<ErrorCodes.ServiceError: 6>, 'OpenAI service failed to generate embeddings', RateLimitError(message='Rate limit reached for microsoft_4x_enterprise_rate_limit in organization org-rocrupyvzgcl4yf25rqq6d1v on requests per min. Limit: 12000 / min. Please try again in 5ms. Contact support@openai.com if you continue to have issues.', http_status=429, request_id=None))"
     ]
    }
   ],
   "source": [
    "print(\"Populating memory...\")\n",
    "await populate_memory(kernel)\n",
    "\n",
    "print(\"Asking questions... (manually)\")\n",
    "await search_memory_examples(kernel)\n",
    "\n",
    "print(\"Setting up a chat (with memory!)\")\n",
    "chat_func, context = await setup_chat_with_memory(kernel)\n",
    "\n",
    "print(\"Begin chatting (type 'exit' to exit):\\n\")\n",
    "chatting = True\n",
    "while chatting:\n",
    "    chatting = await chat(kernel, chat_func, context)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a51542b",
   "metadata": {},
   "source": [
    "### Adding documents to your memory\n",
    "\n",
    "Many times in your applications you'll want to bring in external documents into your memory. Let's see how we can do this using our VolatileMemoryStore.\n",
    "\n",
    "Let's first get some data using some of the links in the Semantic Kernel repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d5a1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "github_files ={}\n",
    "github_files[\"https://github.com/microsoft/semantic-kernel/blob/main/README.md\"] = \\\n",
    "    \"README: Installation, getting started, and how to contribute\"\n",
    "github_files[\"https://github.com/microsoft/semantic-kernel/blob/main/samples/notebooks/dotnet/2-running-prompts-from-file.ipynb\"] = \\\n",
    "    \"Jupyter notebook describing how to pass prompts from a file to a semantic skill or function\"\n",
    "github_files[\"https://github.com/microsoft/semantic-kernel/blob/main/samples/notebooks/dotnet/Getting-Started-Notebook.ipynb\"] = \\\n",
    "    \"Jupyter notebook describing how to get started with the Semantic Kernel\"\n",
    "github_files[\"https://github.com/microsoft/semantic-kernel/tree/main/samples/skills/ChatSkill/ChatGPT\"] = \\\n",
    "    \"Sample demonstrating how to create a chat skill interfacing with ChatGPT\"\n",
    "github_files[\"https://github.com/microsoft/semantic-kernel/blob/main/dotnet/src/SemanticKernel/Memory/Volatile/VolatileMemoryStore.cs\"] = \\\n",
    "    \"C# class that defines a volatile embedding store\"\n",
    "github_files[\"https://github.com/microsoft/semantic-kernel/tree/main/samples/dotnet/KernelHttpServer/README.md\"] = \\\n",
    "    \"README: How to set up a Semantic Kernel Service API using Azure Function Runtime v4\"\n",
    "github_files[\"https://github.com/microsoft/semantic-kernel/tree/main/samples/apps/chat-summary-webapp-react/README.md\"] = \\\n",
    "    \"README: README associated with a sample starter react-based chat summary webapp\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75f3ea5e",
   "metadata": {},
   "source": [
    "Now let's add these files to our VolatileMemoryStore using `SaveReferenceAsync`. We'll separate these memories from the chat memories by putting them in a different memory collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170e7142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding some GitHub file URLs and their descriptions to a volatile Semantic Memory.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'OpenAITextCompletion' object has no attribute 'generate_embeddings_async'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m entry, value \u001b[39min\u001b[39;00m github_files\u001b[39m.\u001b[39mitems():\n\u001b[1;32m----> 4\u001b[0m     \u001b[39mawait\u001b[39;00m kernel\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39msave_reference_async(\n\u001b[0;32m      5\u001b[0m         collection\u001b[39m=\u001b[39mmemory_collection_name,\n\u001b[0;32m      6\u001b[0m         description\u001b[39m=\u001b[39mvalue,\n\u001b[0;32m      7\u001b[0m         text\u001b[39m=\u001b[39mvalue,\n\u001b[0;32m      8\u001b[0m         external_id\u001b[39m=\u001b[39mentry,\n\u001b[0;32m      9\u001b[0m         external_source_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGitHub\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m     )\n\u001b[0;32m     11\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     12\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m  URL \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m saved\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(i))\n",
      "File \u001b[1;32mc:\\Users\\abharris\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\semantic_kernel\\memory\\semantic_text_memory.py:44\u001b[0m, in \u001b[0;36mSemanticTextMemory.save_reference_async\u001b[1;34m(self, collection, text, external_id, external_source_name, description)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39msave_reference_async\u001b[39m(\n\u001b[0;32m     37\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     38\u001b[0m     collection: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     description: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     43\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 44\u001b[0m     embedding \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_embeddings_generator\u001b[39m.\u001b[39;49mgenerate_embeddings_async([text])\n\u001b[0;32m     45\u001b[0m     data \u001b[39m=\u001b[39m MemoryRecord\u001b[39m.\u001b[39mreference_record(\n\u001b[0;32m     46\u001b[0m         external_id, external_source_name, description, embedding\n\u001b[0;32m     47\u001b[0m     )\n\u001b[0;32m     49\u001b[0m     \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_storage\u001b[39m.\u001b[39mput_value_async(collection, external_id, data)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'OpenAITextCompletion' object has no attribute 'generate_embeddings_async'"
     ]
    }
   ],
   "source": [
    "memory_collection_name = \"SKGitHub\"\n",
    "print(\"Adding some GitHub file URLs and their descriptions to a volatile Semantic Memory.\");\n",
    "i = 0\n",
    "for entry, value in github_files.items():\n",
    "    await kernel.memory.save_reference_async(\n",
    "        collection=memory_collection_name,\n",
    "        description=value,\n",
    "        text=value,\n",
    "        external_id=entry,\n",
    "        external_source_name=\"GitHub\"\n",
    "    )\n",
    "    i += 1\n",
    "    print(\"  URL {} saved\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143911c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask = \"I love Jupyter notebooks, how should I get started?\"\n",
    "print(\"===========================\\n\" + \"Query: \" + ask + \"\\n\")\n",
    "\n",
    "memories = await kernel.memory.search_async(memory_collection_name, ask, limit=5, min_relevance_score=0.77)\n",
    "\n",
    "i = 0\n",
    "for memory in memories:\n",
    "    print(\"Result {++i}:\")\n",
    "    print(\"  URL:     : \" + memory.id)\n",
    "    print(\"  Title    : \" + memory.description)\n",
    "    print(\"  Relevance: \" + str(memory.relevance))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59294dac",
   "metadata": {},
   "source": [
    "Now you might be wondering what happens if you have so much data that it doesn't fit into your RAM? That's where you want to make use of an external Vector Database made specifically for storing and retrieving embeddings.\n",
    "\n",
    "Stay tuned for that!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
